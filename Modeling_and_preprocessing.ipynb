{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Innovation-On-Blockchain/Data-Modeling/blob/main/Modeling_and_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": "#TODO: Data Processing\n# ==============================================================\n# Input:  raw/transactions_raw.parquet, labels/labels.parquet,\n#         processed/nodes.parquet (for contract filtering)\n# Output: processed/formatted_transactions.parquet\n#         processed/node_labels.parquet\n#         processed/data_splits.json\n# ==============================================================\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport pandas as pd\nimport json\nimport gc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMl3_b96c--c",
    "outputId": "aeba434b-f315-4439-c9ae-92792a2dc373"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import os\nimport pyarrow.parquet as pq\n\nshared_folder_path = '/content/drive/MyDrive/Math_168_Folder'\nos.chdir(shared_folder_path)\nDATA_DIR = \"/content/drive/MyDrive/Math_168_Folder/data\"\nOUTPUT_DIR = os.path.join(DATA_DIR, \"processed\")\n\n# Input files (from Rust pipeline)\nTRANSACTIONS_PATH = os.path.join(DATA_DIR, \"raw\", \"transactions_raw.parquet\")\nLABELS_PATH       = os.path.join(DATA_DIR, \"labels\", \"labels.parquet\")\nNODES_PATH        = os.path.join(DATA_DIR, \"processed\", \"nodes.parquet\")\n\nprint(f\"File size: {os.path.getsize(TRANSACTIONS_PATH) / 1e9:.2f} GB\")\n\nmeta = pq.read_metadata(TRANSACTIONS_PATH)\nprint(f\"Rows: {meta.num_rows:,}\")\nprint(f\"Columns: {meta.num_columns}\")",
   "metadata": {
    "id": "Ti2ziOd9pdhl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0f5a20ac-721d-4132-c482-921cf974a082"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0witpEDHwiK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f1613238-4e9a-48e1-eb37-51539fe22c96"
   },
   "outputs": [],
   "source": "# ================================================================\n# Load raw data\n# ================================================================\n# Read the two input parquet files produced by the Rust pipeline.\n# transactions_raw columns: tx_hash, block_timestamp, from_address,\n#                           to_address, value_wei  (all strings)\n# labels columns: address, label, label_source, source_url, retrieved_at\n# ================================================================\n\nchunks = []\nparquet_file = pq.ParquetFile(TRANSACTIONS_PATH)\n\nfor batch in parquet_file.iter_batches(batch_size=1_000_000):\n    chunk = batch.to_pandas()\n    # Filter invalid rows immediately to save memory\n    chunk = chunk[\n        chunk[\"from_address\"].notna() & (chunk[\"from_address\"] != \"\") &\n        chunk[\"to_address\"].notna()   & (chunk[\"to_address\"]   != \"\")\n    ]\n    chunks.append(chunk)\n    print(f\"  Processed batch — kept {len(chunk):,} rows\", end=\"\\r\")\n\ndf = pd.concat(chunks, ignore_index=True)\ndel chunks\ngc.collect()\n\nprint(f\"\\nLoaded {len(df):,} valid rows\")\n\nlabels = pd.read_parquet(LABELS_PATH)\n\nprint(f\"Raw transactions loaded: {len(df):,} rows\")\nprint(f\"Labels loaded:           {len(labels):,} sanctioned addresses\")\nprint()\n\n# Quick data-quality report\nn_empty_from = (df[\"from_address\"].isna() | (df[\"from_address\"] == \"\")).sum()\nn_empty_to   = (df[\"to_address\"].isna()   | (df[\"to_address\"]   == \"\")).sum()\nprint(f\"Empty/null from_address: {n_empty_from:,} / {len(df):,}\")\nprint(f\"Empty/null to_address:   {n_empty_to:,} / {len(df):,}\")\nprint()"
  },
  {
   "cell_type": "code",
   "source": "# ================================================================\n# Step 1: Filter Out Contract Addresses\n# ================================================================\n# Load nodes.parquet from the Rust pipeline (Stage 4: build-nodes).\n# It contains: address, is_contract (bool), degree_in, degree_out, node_type.\n# Remove all transaction rows where EITHER from_address or to_address\n# is a smart contract. This keeps only EOA-to-EOA transactions.\n# ================================================================\n\nnodes_pf = pq.ParquetFile(NODES_PATH)\n\n# Only read the columns we need to save memory\ncontract_addresses = set()\nfor batch in nodes_pf.iter_batches(batch_size=1_000_000, columns=[\"address\", \"is_contract\"]):\n    chunk = batch.to_pandas()\n    contracts = chunk.loc[chunk[\"is_contract\"] == True, \"address\"]\n    contract_addresses.update(contracts)\n    print(f\"  Scanned batch — found {len(contract_addresses):,} contracts so far\", end=\"\\r\")\n\nprint(f\"\\nTotal contract addresses found: {len(contract_addresses):,}\")\n\nbefore = len(df)\ndf = df[\n    ~df[\"from_address\"].isin(contract_addresses) &\n    ~df[\"to_address\"].isin(contract_addresses)\n]\ndf = df.reset_index(drop=True)\nafter = len(df)\n\nprint(f\"\\nStep 1 — Filter contract addresses\")\nprint(f\"  Before: {before:,}  After: {after:,}  Removed: {before - after:,}\")\nprint(f\"  Percentage removed: {(before - after) / before * 100:.2f}%\")\nprint()\n\ndel contract_addresses\ngc.collect()",
   "metadata": {
    "id": "QAQSFfcsadc5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from google.colab import drive\ndrive.mount('/content/drive')",
   "metadata": {
    "id": "794mad66oO40",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "862de43a-1379-4303-e8e8-8901d7e74221"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 2: Create Address-to-ID Mapping\n",
    "# ================================================================\n",
    "# Collect every unique address from both from_address and to_address.\n",
    "# Sort them so the mapping is deterministic, then assign each one\n",
    "# a numeric ID starting from 0. Add from_id / to_id columns.\n",
    "# ================================================================\n",
    "\n",
    "all_addresses = pd.concat([df[\"from_address\"], df[\"to_address\"]]).unique()\n",
    "address_to_id = {addr: i for i, addr in enumerate(sorted(all_addresses))}\n",
    "\n",
    "df[\"from_id\"] = df[\"from_address\"].map(address_to_id)\n",
    "df[\"to_id\"]   = df[\"to_address\"].map(address_to_id)\n",
    "\n",
    "print(f\"Step 2 — Address-to-ID mapping\")\n",
    "print(f\"  Unique addresses: {len(address_to_id):,}\")\n",
    "print(f\"  ID range: 0 .. {len(address_to_id) - 1}\")\n",
    "print()"
   ],
   "metadata": {
    "id": "Q_hbptc1admQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b032fb78-548e-446d-9d0f-a46bd661d33e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 2 — Address-to-ID mapping\n",
      "  Unique addresses: 33,895,535\n",
      "  ID range: 0 .. 33895534\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 3: Convert Timestamps\n",
    "# ================================================================\n",
    "# Parse block_timestamp from ISO-8601 strings to datetime objects.\n",
    "# Sort the dataframe by timestamp ascending (earliest first).\n",
    "# Subtract the earliest timestamp so the first row has Timestamp=0\n",
    "# and all other values are seconds elapsed since that first tx.\n",
    "# ================================================================\n",
    "\n",
    "df[\"block_timestamp\"] = pd.to_datetime(df[\"block_timestamp\"])\n",
    "df = df.sort_values(\"block_timestamp\").reset_index(drop=True)\n",
    "\n",
    "t_min = df[\"block_timestamp\"].min()\n",
    "df[\"Timestamp\"] = (df[\"block_timestamp\"] - t_min).dt.total_seconds().astype(int)\n",
    "\n",
    "print(f\"Step 3 — Convert timestamps\")\n",
    "print(f\"  Earliest: {t_min}\")\n",
    "print(f\"  Latest:   {df['block_timestamp'].max()}\")\n",
    "print(f\"  Span:     {df['Timestamp'].max():,} seconds\")\n",
    "print(f\"  First Timestamp value: {df['Timestamp'].iloc[0]}\")\n",
    "print()\n",
    "\n",
    "df.drop(columns=[\"block_timestamp\"], inplace=True)\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "A4-jqebmadol",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "45d8da5d-2b81-4d2f-ab0b-8aa86328259d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 3 — Convert timestamps\n",
      "  Earliest: 2015-08-07 05:01:09+00:00\n",
      "  Latest:   2026-02-19 19:21:47+00:00\n",
      "  Span:     332,605,238 seconds\n",
      "  First Timestamp value: 0\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 4: Convert Values (Wei -> ETH)\n",
    "# ================================================================\n",
    "# value_wei is a string in wei (1 ETH = 1e18 wei).\n",
    "# Convert to float ETH. Store as both \"Amount Sent\" and\n",
    "# \"Amount Received\" — they are equal for direct ETH transfers.\n",
    "# ================================================================\n",
    "\n",
    "WEI_PER_ETH = 10**18\n",
    "\n",
    "df[\"Amount Sent\"]     = df[\"value_wei\"].astype(float) / WEI_PER_ETH\n",
    "df[\"Amount Received\"] = df[\"Amount Sent\"]\n",
    "\n",
    "print(f\"Step 4 — Wei to ETH conversion\")\n",
    "print(f\"  Min:  {df['Amount Sent'].min():.8f} ETH\")\n",
    "print(f\"  Max:  {df['Amount Sent'].max():.8f} ETH\")\n",
    "print(f\"  Mean: {df['Amount Sent'].mean():.8f} ETH\")\n",
    "print()\n",
    "\n",
    "df.drop(columns=[\"value_wei\"], inplace=True)\n",
    "gc.collect()\n"
   ],
   "metadata": {
    "id": "FVGu7ozBadrM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6afda9f0-2872-44b8-be5f-081826d87c58"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 4 — Wei to ETH conversion\n",
      "  Min:  0.00000000 ETH\n",
      "  Max:  1400000.00000000 ETH\n",
      "  Mean: 14.01213088 ETH\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 5: Add Edge Labels\n",
    "# ================================================================\n",
    "# Build a set of the 82 OFAC-sanctioned addresses from labels.\n",
    "# For each transaction edge:\n",
    "#   - Is Laundering = 1  if from_address OR to_address is sanctioned\n",
    "#   - Is Laundering = 0  if NEITHER endpoint is sanctioned\n",
    "# Edges between two clean addresses are intentionally kept as 0 —\n",
    "# they give the GNN examples of normal graph structure.\n",
    "# ================================================================\n",
    "\n",
    "sanctioned_set = set(labels[\"address\"])\n",
    "\n",
    "df[\"Is Laundering\"] = (\n",
    "    df[\"from_address\"].isin(sanctioned_set) |\n",
    "    df[\"to_address\"].isin(sanctioned_set)\n",
    ").astype(int)\n",
    "\n",
    "n_launder = (df[\"Is Laundering\"] == 1).sum()\n",
    "n_clean   = (df[\"Is Laundering\"] == 0).sum()\n",
    "print(f\"Step 5 — Edge labels\")\n",
    "print(f\"  Sanctioned addresses: {len(sanctioned_set)}\")\n",
    "print(f\"  Laundering edges (1): {n_launder:,}\")\n",
    "print(f\"  Clean edges (0):      {n_clean:,}\")\n",
    "print()\n",
    "\n",
    "df.drop(columns=[\"from_address\", \"to_address\"], inplace=True)\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "6Or1258Hadtb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "73489bca-f166-4ea0-dadc-b33501bbf910"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 5 — Edge labels\n",
      "  Sanctioned addresses: 82\n",
      "  Laundering edges (1): 10,453\n",
      "  Clean edges (0):      90,717,710\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# ================================================================\n# Step 6: Build Formatted Transactions File\n# ================================================================\n# Assemble the final dataframe with columns in this order:\n#   EdgeID, from_id, to_id, Timestamp, Amount Sent, Sent Currency,\n#   Amount Received, Received Currency, Payment Format, Is Laundering\n#\n# - EdgeID = row index starting at 0\n# - Sent Currency, Received Currency, Payment Format = always 1\n#   (single-chain ETH data; kept for IBM Multi-GNN format compatibility)\n#\n# Save to: OUTPUT_DIR/formatted_transactions.parquet\n# ================================================================\n\nSENT_CURRENCY_CODE = 1\nRECEIVED_CURRENCY_CODE = 1\nPAYMENT_FORMAT_CODE = 1\n\nformatted = pd.DataFrame({\n    \"EdgeID\":            range(len(df)),\n    \"from_id\":           df[\"from_id\"].astype(int),\n    \"to_id\":             df[\"to_id\"].astype(int),\n    \"Timestamp\":         df[\"Timestamp\"].astype(int),\n    \"Amount Sent\":       df[\"Amount Sent\"].astype(float),\n    \"Sent Currency\":     SENT_CURRENCY_CODE,\n    \"Amount Received\":   df[\"Amount Received\"].astype(float),\n    \"Received Currency\": RECEIVED_CURRENCY_CODE,\n    \"Payment Format\":    PAYMENT_FORMAT_CODE,\n    \"Is Laundering\":     df[\"Is Laundering\"].astype(int),\n})\n\nformatted_path = os.path.join(OUTPUT_DIR, \"formatted_transactions.parquet\")\nformatted.to_parquet(formatted_path, index=False)\n\nprint(\"Step 6 — Formatted transactions saved\")\nprint(f\"  Path: {formatted_path}\")\nprint(f\"  Rows: {len(formatted):,}\")\nprint(f\"  Columns: {list(formatted.columns)}\")\nprint()\n\nn_edges = len(formatted)  # save for Step 8\n\ndel df, formatted\ngc.collect()\nprint(\"Memory freed before Step 7\")",
   "metadata": {
    "id": "kmMnqOIIadvw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7e31c529-a2f0-4800-9916-d045923db562"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ================================================================\n# Step 7: Build Node Labels File\n# ================================================================\n# For each address in address_to_id:\n#   - node_id = address_to_id[address]\n#   - is_sanctioned = 1 if address in sanctioned_set, else 0\n#\n# Since contracts were filtered in Step 1, only EOA addresses remain.\n#\n# Columns: node_id, is_sanctioned\n# Save to: OUTPUT_DIR/node_labels.parquet\n# ================================================================\n\nnode_labels = pd.DataFrame({\n    \"node_id\": list(address_to_id.values()),\n    \"is_sanctioned\": [1 if addr in sanctioned_set else 0 for addr in address_to_id.keys()],\n}).sort_values(\"node_id\").reset_index(drop=True)\n\nnode_labels_path = os.path.join(OUTPUT_DIR, \"node_labels.parquet\")\nnode_labels.to_parquet(node_labels_path, index=False)\n\nn_sanctioned = node_labels['is_sanctioned'].sum()\nn_total = len(node_labels)\nratio = (n_total - n_sanctioned) / max(n_sanctioned, 1)\n\nprint(\"Step 7 — Node labels saved\")\nprint(f\"  Path: {node_labels_path}\")\nprint(f\"  Rows (nodes): {n_total:,}\")\nprint(f\"  Sanctioned nodes: {n_sanctioned:,}\")\nprint(f\"  Non-sanctioned nodes: {n_total - n_sanctioned:,}\")\nprint(f\"  Class imbalance ratio: {ratio:,.0f}:1\")\nprint()",
   "metadata": {
    "id": "99vEuEsaan1W",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "17d42439-0547-44c6-a126-6585bd58b59a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ================================================================\n# Step 8: Create Train/Validation Split\n# ================================================================\n# Data is sorted by time (Step 3). Temporal split:\n#   - Train = first 80% of edges (chronologically earliest)\n#   - Val   = last 20% of edges (chronologically latest)\n# This prevents temporal data leakage.\n#\n# No test split — in a transfer learning setup the test set comes\n# from a different domain (different blockchain).\n#\n# Save as: OUTPUT_DIR/data_splits.json\n# ================================================================\n\nsplit_point = int(n_edges * 0.8)\ntrain_edge_ids = list(range(0, split_point))\nval_edge_ids   = list(range(split_point, n_edges))\n\nsplits = {\n    \"train_edge_ids\": train_edge_ids,\n    \"val_edge_ids\": val_edge_ids,\n}\n\nsplits_path = os.path.join(OUTPUT_DIR, \"data_splits.json\")\nwith open(splits_path, \"w\") as f:\n    json.dump(splits, f)\n\nprint(\"Step 8 — Data splits saved\")\nprint(f\"  Path: {splits_path}\")\nprint(f\"  Train edges: {len(train_edge_ids):,} ({len(train_edge_ids)/n_edges*100:.1f}%)\")\nprint(f\"  Val edges:   {len(val_edge_ids):,} ({len(val_edge_ids)/n_edges*100:.1f}%)\")\nprint()",
   "metadata": {
    "id": "HCFQwbESasYi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "52d71784-5f24-4643-a3ab-def67ee8ff29"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "BcDA9JdjZqBb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pLzdh15TZklk"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
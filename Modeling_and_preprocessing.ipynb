{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#TODO: Data Processing\n",
    "# ==============================================================\n",
    "# Input:  raw/transactions_raw.parquet, labels/labels.parquet,\n",
    "#         processed/nodes.parquet (for contract filtering)\n",
    "# Output: processed/formatted_transactions.parquet\n",
    "#         processed/node_labels.parquet\n",
    "#         processed/data_splits.json\n",
    "# ==============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import gc"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMl3_b96c--c",
    "outputId": "e46260cb-56eb-4ea1-89b0-132e4d6767d9"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "shared_folder_path = '/content/drive/MyDrive/Math_168_Folder'\n",
    "os.chdir(shared_folder_path)\n",
    "DATA_DIR = \"/content/drive/MyDrive/Math_168_Folder/data\"\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "\n",
    "# Input files (from Rust pipeline)\n",
    "TRANSACTIONS_PATH = os.path.join(DATA_DIR, \"raw\", \"transactions_raw.parquet\")\n",
    "LABELS_PATH       = os.path.join(DATA_DIR, \"labels\", \"labels.parquet\")\n",
    "NODES_PATH        = os.path.join(DATA_DIR, \"processed\", \"nodes.parquet\")\n",
    "\n",
    "print(f\"File size: {os.path.getsize(TRANSACTIONS_PATH) / 1e9:.2f} GB\")\n",
    "\n",
    "meta = pq.read_metadata(TRANSACTIONS_PATH)\n",
    "print(f\"Rows: {meta.num_rows:,}\")\n",
    "print(f\"Columns: {meta.num_columns}\")"
   ],
   "metadata": {
    "id": "Ti2ziOd9pdhl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9ce96313-d237-45d6-8958-58c33f0ce4d9"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File size: 5.58 GB\n",
      "Rows: 90,728,163\n",
      "Columns: 5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W0witpEDHwiK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6f888e01-b074-4bbd-c23a-c909c29a0e7e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Loaded 90,728,163 valid rows\n",
      "Raw transactions loaded: 90,728,163 rows\n",
      "Labels loaded:           82 sanctioned addresses\n",
      "\n",
      "Empty/null from_address: 0 / 90,728,163\n",
      "Empty/null to_address:   0 / 90,728,163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Load raw data\n",
    "# ================================================================\n",
    "# Read the two input parquet files produced by the Rust pipeline.\n",
    "# transactions_raw columns: tx_hash, block_timestamp, from_address,\n",
    "#                           to_address, value_wei  (all strings)\n",
    "# labels columns: address, label, label_source, source_url, retrieved_at\n",
    "# ================================================================\n",
    "\n",
    "chunks = []\n",
    "parquet_file = pq.ParquetFile(TRANSACTIONS_PATH)\n",
    "\n",
    "for batch in parquet_file.iter_batches(batch_size=1_000_000):\n",
    "    chunk = batch.to_pandas()\n",
    "    # Filter invalid rows immediately to save memory\n",
    "    chunk = chunk[\n",
    "        chunk[\"from_address\"].notna() & (chunk[\"from_address\"] != \"\") &\n",
    "        chunk[\"to_address\"].notna()   & (chunk[\"to_address\"]   != \"\")\n",
    "    ]\n",
    "    chunks.append(chunk)\n",
    "    print(f\"  Processed batch — kept {len(chunk):,} rows\", end=\"\\r\")\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nLoaded {len(df):,} valid rows\")\n",
    "\n",
    "labels = pd.read_parquet(LABELS_PATH)\n",
    "\n",
    "print(f\"Raw transactions loaded: {len(df):,} rows\")\n",
    "print(f\"Labels loaded:           {len(labels):,} sanctioned addresses\")\n",
    "print()\n",
    "\n",
    "# Quick data-quality report\n",
    "n_empty_from = (df[\"from_address\"].isna() | (df[\"from_address\"] == \"\")).sum()\n",
    "n_empty_to   = (df[\"to_address\"].isna()   | (df[\"to_address\"]   == \"\")).sum()\n",
    "print(f\"Empty/null from_address: {n_empty_from:,} / {len(df):,}\")\n",
    "print(f\"Empty/null to_address:   {n_empty_to:,} / {len(df):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 1: Filter Out Contract Addresses\n",
    "# ================================================================\n",
    "# Load nodes.parquet from the Rust pipeline (Stage 4: build-nodes).\n",
    "# It contains: address, is_contract (bool), degree_in, degree_out, node_type.\n",
    "# Remove all transaction rows where EITHER from_address or to_address\n",
    "# is a smart contract. This keeps only EOA-to-EOA transactions.\n",
    "# ================================================================\n",
    "\n",
    "nodes_pf = pq.ParquetFile(NODES_PATH)\n",
    "\n",
    "# Only read the columns we need to save memory\n",
    "contract_addresses = set()\n",
    "for batch in nodes_pf.iter_batches(batch_size=1_000_000, columns=[\"address\", \"is_contract\"]):\n",
    "    chunk = batch.to_pandas()\n",
    "    contracts = chunk.loc[chunk[\"is_contract\"] == True, \"address\"]\n",
    "    contract_addresses.update(contracts)\n",
    "    print(f\"  Scanned batch — found {len(contract_addresses):,} contracts so far\", end=\"\\r\")\n",
    "\n",
    "print(f\"\\nTotal contract addresses found: {len(contract_addresses):,}\")\n",
    "\n",
    "before = len(df)\n",
    "df = df[\n",
    "    ~df[\"from_address\"].isin(contract_addresses) &\n",
    "    ~df[\"to_address\"].isin(contract_addresses)\n",
    "]\n",
    "df = df.reset_index(drop=True)\n",
    "after = len(df)\n",
    "\n",
    "print(f\"\\nStep 1 — Filter contract addresses\")\n",
    "print(f\"  Before: {before:,}  After: {after:,}  Removed: {before - after:,}\")\n",
    "print(f\"  Percentage removed: {(before - after) / before * 100:.2f}%\")\n",
    "print()\n",
    "\n",
    "del contract_addresses\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "QAQSFfcsadc5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "65cfaf65-dd97-4ed4-82e7-7de586f1e693"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Scanned batch — found 721,891 contracts so far\n",
      "Total contract addresses found: 721,891\n",
      "\n",
      "Step 1 — Filter contract addresses\n",
      "  Before: 90,728,163  After: 69,657,566  Removed: 21,070,597\n",
      "  Percentage removed: 23.22%\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# ================================================================\n# Step 1.5: Aggressive EOA Pruning\n# ================================================================\n# After contract filtering, ~30M EOA addresses remain — far too many\n# for GNN training. This step removes low-degree EOAs to bring the\n# graph down to ~TARGET_NODES unique addresses.\n#\n# Protection hierarchy (never pruned):\n#   1. Sanctioned addresses (82 OFAC)\n#   2. 1-hop counterparties of sanctioned addresses\n#   3. Hubs (top 0.1% by total degree, from nodes.parquet)\n#\n# All other EOAs are pruned if their total_degree (degree_in +\n# degree_out from nodes.parquet) is below an auto-computed threshold.\n#\n# Ref: Elliptic++ (Elmougy & Liu, KDD'23) uses ~822K addresses.\n# ================================================================\nimport numpy as np\n\n# ---- Configuration ----\nTARGET_NODES = 1_000_000    # Target unique addresses after pruning\nMIN_DEGREE = None           # Set to an int for explicit threshold (overrides TARGET_NODES)\n\n# 1. Collect all unique addresses currently in df\nall_addrs_in_df = set(\n    pd.concat([df[\"from_address\"], df[\"to_address\"]]).unique()\n)\nprint(f\"Unique addresses in df (post-contract-filter): {len(all_addrs_in_df):,}\")\n\n# 2. Load degree + node_type from nodes.parquet (only for addresses in df)\ndegree_lookup = {}\nhub_set = set()\nnodes_pf = pq.ParquetFile(NODES_PATH)\nfor batch in nodes_pf.iter_batches(\n        batch_size=2_000_000,\n        columns=[\"address\", \"degree_in\", \"degree_out\", \"node_type\"]):\n    chunk = batch.to_pandas()\n    mask = chunk[\"address\"].isin(all_addrs_in_df)\n    relevant = chunk[mask]\n    if len(relevant) > 0:\n        batch_degrees = dict(zip(\n            relevant[\"address\"].values,\n            (relevant[\"degree_in\"] + relevant[\"degree_out\"]).values.astype(int)\n        ))\n        degree_lookup.update(batch_degrees)\n        hub_batch = relevant.loc[relevant[\"node_type\"] == \"hub\", \"address\"]\n        hub_set.update(hub_batch.values)\n    print(f\"  Scanned nodes — {len(degree_lookup):,} degrees, {len(hub_set):,} hubs\", end=\"\\r\")\n\nprint(f\"\\nDegree data: {len(degree_lookup):,} addresses, {len(hub_set):,} hubs\")\n\n# 3. Sanctioned set (reuse `labels` loaded in cell-2)\nsanctioned_set = set(labels[\"address\"])\n\n# 4. Derive 1-hop counterparties of sanctioned addresses from df\nsanctioned_mask = (\n    df[\"from_address\"].isin(sanctioned_set) |\n    df[\"to_address\"].isin(sanctioned_set)\n)\nsanctioned_edges = df[sanctioned_mask]\ncounterparty_set = (\n    set(sanctioned_edges[\"from_address\"].values) |\n    set(sanctioned_edges[\"to_address\"].values)\n)\nn_positive_edges = len(sanctioned_edges)\nprint(f\"1-hop counterparties of sanctioned: {len(counterparty_set):,}\")\nprint(f\"Positive edges (touching sanctioned): {n_positive_edges:,}\")\ndel sanctioned_edges\n\n# 5. Protected set = sanctioned ∪ counterparties ∪ hubs\nprotected_set = sanctioned_set | counterparty_set | hub_set\nprotected_in_df = protected_set & all_addrs_in_df\nprint(f\"\\nProtected addresses (in df): {len(protected_in_df):,}\")\nprint(f\"  Sanctioned:     {len(sanctioned_set & all_addrs_in_df):>8,}\")\nprint(f\"  Counterparties: {len(counterparty_set & all_addrs_in_df):>8,}\")\nprint(f\"  Hubs:           {len(hub_set & all_addrs_in_df):>8,}\")\n\n# 6. Build sorted degree array for non-protected addresses\nnon_protected = [a for a in all_addrs_in_df if a not in protected_set]\nnon_protected_degrees = np.array(\n    [degree_lookup.get(a, 0) for a in non_protected], dtype=np.int64\n)\nnon_protected_degrees.sort()\nn_protected = len(protected_in_df)\n\n# Print degree distribution summary\npcts = [0, 25, 50, 75, 90, 95, 99, 100]\nif len(non_protected_degrees) > 0:\n    print(f\"\\nNon-protected EOA degree distribution ({len(non_protected):,} addresses):\")\n    for p in pcts:\n        val = int(np.percentile(non_protected_degrees, p))\n        print(f\"  {p:>3}th percentile: {val:,}\")\n\n# 7. Find degree threshold\nif MIN_DEGREE is not None:\n    threshold = MIN_DEGREE\n    idx = np.searchsorted(non_protected_degrees, threshold, side=\"left\")\n    n_surviving_unprotected = len(non_protected_degrees) - idx\n    total_expected = n_protected + n_surviving_unprotected\n    print(f\"\\nUsing explicit MIN_DEGREE = {threshold}\")\n    print(f\"Expected surviving addresses: {total_expected:,}\")\nelse:\n    if len(non_protected_degrees) == 0 or n_protected >= TARGET_NODES:\n        threshold = 1\n        print(f\"\\nProtected set ({n_protected:,}) already meets target ({TARGET_NODES:,})\")\n    else:\n        lo, hi = 2, int(non_protected_degrees[-1]) + 1\n        while lo < hi:\n            mid = (lo + hi) // 2\n            idx = np.searchsorted(non_protected_degrees, mid, side=\"left\")\n            n_surviving = len(non_protected_degrees) - idx\n            total = n_protected + n_surviving\n            if total > TARGET_NODES:\n                lo = mid + 1\n            else:\n                hi = mid\n        threshold = lo\n\n        idx = np.searchsorted(non_protected_degrees, threshold, side=\"left\")\n        n_surviving_unprotected = len(non_protected_degrees) - idx\n        total_expected = n_protected + n_surviving_unprotected\n\n        # Check if threshold-1 is closer to target\n        if threshold > 2:\n            idx_lower = np.searchsorted(non_protected_degrees, threshold - 1, side=\"left\")\n            n_lower = len(non_protected_degrees) - idx_lower\n            total_lower = n_protected + n_lower\n            if abs(total_lower - TARGET_NODES) < abs(total_expected - TARGET_NODES):\n                threshold -= 1\n                total_expected = total_lower\n\n        print(f\"\\nAuto-computed degree threshold: {threshold}\")\n        print(f\"Expected surviving addresses: {total_expected:,} (target: {TARGET_NODES:,})\")\n\n# 8. Build prune set\nprune_set = frozenset(\n    addr for addr in non_protected\n    if degree_lookup.get(addr, 0) < threshold\n)\nprint(f\"Addresses to prune: {len(prune_set):,}\")\n\n# Free intermediate structures before the heavy filter\ndel non_protected, non_protected_degrees, degree_lookup\ngc.collect()\n\n# 9. Filter df — remove rows where either endpoint is pruned\nbefore_rows = len(df)\nbefore_addrs = len(all_addrs_in_df)\n\ndf = df[\n    ~df[\"from_address\"].isin(prune_set) &\n    ~df[\"to_address\"].isin(prune_set)\n]\ndf = df.reset_index(drop=True)\n\nafter_rows = len(df)\nremaining_addrs = set(\n    pd.concat([df[\"from_address\"], df[\"to_address\"]]).unique()\n)\nafter_addrs = len(remaining_addrs)\n\n# 10. Verification\nsanctioned_before = sanctioned_set & all_addrs_in_df\nsanctioned_after = sanctioned_set & remaining_addrs\n\nprint(f\"\\n{'='*55}\")\nprint(f\" Step 1.5 — Aggressive EOA Pruning Complete\")\nprint(f\"{'='*55}\")\nprint(f\"  Degree threshold:      {threshold}\")\nprint(f\"  Addresses: {before_addrs:>12,} → {after_addrs:>12,}  \"\n      f\"(kept {after_addrs/before_addrs*100:.2f}%)\")\nprint(f\"  Transactions: {before_rows:>10,} → {after_rows:>10,}  \"\n      f\"(kept {after_rows/before_rows*100:.2f}%)\")\nprint(f\"  Sanctioned:  {len(sanctioned_before):>4,} → {len(sanctioned_after):>4,}\")\n\n# Verify positive edge preservation\npositive_after = (\n    df[\"from_address\"].isin(sanctioned_set) |\n    df[\"to_address\"].isin(sanctioned_set)\n).sum()\nprint(f\"  Positive edges: {n_positive_edges:>8,} → {positive_after:>8,}\")\nprint()\n\n# HARD ASSERTION: no sanctioned node was lost\nassert sanctioned_after == sanctioned_before, \\\n    f\"FATAL: Sanctioned nodes lost: {sanctioned_before - sanctioned_after}\"\nprint(\"  ✓ All sanctioned nodes preserved\")\nassert positive_after == n_positive_edges, \\\n    f\"FATAL: Positive edges lost: {n_positive_edges} → {positive_after}\"\nprint(\"  ✓ All positive edges preserved\")\n\n# Clean up\ndel prune_set, all_addrs_in_df, remaining_addrs, hub_set, counterparty_set\ndel protected_set, protected_in_df, sanctioned_before, sanctioned_after\ngc.collect()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "794mad66oO40",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c7800611-4d83-41bf-bdb5-793fade5ac1d"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 2: Create Address-to-ID Mapping\n",
    "# ================================================================\n",
    "# Collect every unique address from both from_address and to_address.\n",
    "# Sort them so the mapping is deterministic, then assign each one\n",
    "# a numeric ID starting from 0. Add from_id / to_id columns.\n",
    "# ================================================================\n",
    "\n",
    "all_addresses = pd.concat([df[\"from_address\"], df[\"to_address\"]]).unique()\n",
    "address_to_id = {addr: i for i, addr in enumerate(sorted(all_addresses))}\n",
    "\n",
    "df[\"from_id\"] = df[\"from_address\"].map(address_to_id)\n",
    "df[\"to_id\"]   = df[\"to_address\"].map(address_to_id)\n",
    "\n",
    "print(f\"Step 2 — Address-to-ID mapping\")\n",
    "print(f\"  Unique addresses: {len(address_to_id):,}\")\n",
    "print(f\"  ID range: 0 .. {len(address_to_id) - 1}\")\n",
    "print()"
   ],
   "metadata": {
    "id": "Q_hbptc1admQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9337a64c-7551-43c5-e0e1-b1e64fa4029d"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 2 — Address-to-ID mapping\n",
      "  Unique addresses: 30,378,276\n",
      "  ID range: 0 .. 30378275\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 3: Convert Timestamps\n",
    "# ================================================================\n",
    "# Parse block_timestamp from ISO-8601 strings to datetime objects.\n",
    "# Sort the dataframe by timestamp ascending (earliest first).\n",
    "# Subtract the earliest timestamp so the first row has Timestamp=0\n",
    "# and all other values are seconds elapsed since that first tx.\n",
    "# ================================================================\n",
    "\n",
    "df[\"block_timestamp\"] = pd.to_datetime(df[\"block_timestamp\"])\n",
    "df = df.sort_values(\"block_timestamp\").reset_index(drop=True)\n",
    "\n",
    "t_min = df[\"block_timestamp\"].min()\n",
    "df[\"Timestamp\"] = (df[\"block_timestamp\"] - t_min).dt.total_seconds().astype(int)\n",
    "\n",
    "print(f\"Step 3 — Convert timestamps\")\n",
    "print(f\"  Earliest: {t_min}\")\n",
    "print(f\"  Latest:   {df['block_timestamp'].max()}\")\n",
    "print(f\"  Span:     {df['Timestamp'].max():,} seconds\")\n",
    "print(f\"  First Timestamp value: {df['Timestamp'].iloc[0]}\")\n",
    "print()\n",
    "\n",
    "df.drop(columns=[\"block_timestamp\"], inplace=True)\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "A4-jqebmadol",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "56a91308-6f99-4d9b-ea76-6040fe4fd9bf"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 3 — Convert timestamps\n",
      "  Earliest: 2015-08-07 05:01:09+00:00\n",
      "  Latest:   2026-02-19 19:21:47+00:00\n",
      "  Span:     332,605,238 seconds\n",
      "  First Timestamp value: 0\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 4: Convert Values (Wei -> ETH)\n",
    "# ================================================================\n",
    "# value_wei is a string in wei (1 ETH = 1e18 wei).\n",
    "# Convert to float ETH. Store as both \"Amount Sent\" and\n",
    "# \"Amount Received\" — they are equal for direct ETH transfers.\n",
    "# ================================================================\n",
    "\n",
    "WEI_PER_ETH = 10**18\n",
    "\n",
    "df[\"Amount Sent\"]     = df[\"value_wei\"].astype(float) / WEI_PER_ETH\n",
    "df[\"Amount Received\"] = df[\"Amount Sent\"]\n",
    "\n",
    "print(f\"Step 4 — Wei to ETH conversion\")\n",
    "print(f\"  Min:  {df['Amount Sent'].min():.8f} ETH\")\n",
    "print(f\"  Max:  {df['Amount Sent'].max():.8f} ETH\")\n",
    "print(f\"  Mean: {df['Amount Sent'].mean():.8f} ETH\")\n",
    "print()\n",
    "\n",
    "df.drop(columns=[\"value_wei\"], inplace=True)\n",
    "gc.collect()\n"
   ],
   "metadata": {
    "id": "FVGu7ozBadrM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "48e0e085-caba-4284-bd55-b5b900fb86e8"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 4 — Wei to ETH conversion\n",
      "  Min:  0.00000000 ETH\n",
      "  Max:  1400000.00000000 ETH\n",
      "  Mean: 14.61243847 ETH\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 5: Add Edge Labels\n",
    "# ================================================================\n",
    "# Build a set of the 82 OFAC-sanctioned addresses from labels.\n",
    "# For each transaction edge:\n",
    "#   - Is Laundering = 1  if from_address OR to_address is sanctioned\n",
    "#   - Is Laundering = 0  if NEITHER endpoint is sanctioned\n",
    "# Edges between two clean addresses are intentionally kept as 0 —\n",
    "# they give the GNN examples of normal graph structure.\n",
    "# ================================================================\n",
    "\n",
    "sanctioned_set = set(labels[\"address\"])\n",
    "\n",
    "df[\"Is Laundering\"] = (\n",
    "    df[\"from_address\"].isin(sanctioned_set) |\n",
    "    df[\"to_address\"].isin(sanctioned_set)\n",
    ").astype(int)\n",
    "\n",
    "n_launder = (df[\"Is Laundering\"] == 1).sum()\n",
    "n_clean   = (df[\"Is Laundering\"] == 0).sum()\n",
    "print(f\"Step 5 — Edge labels\")\n",
    "print(f\"  Sanctioned addresses: {len(sanctioned_set)}\")\n",
    "print(f\"  Laundering edges (1): {n_launder:,}\")\n",
    "print(f\"  Clean edges (0):      {n_clean:,}\")\n",
    "print()\n",
    "\n",
    "df.drop(columns=[\"from_address\", \"to_address\"], inplace=True)\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "6Or1258Hadtb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "040d7a34-f140-49e9-9024-9691b09456eb"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 5 — Edge labels\n",
      "  Sanctioned addresses: 82\n",
      "  Laundering edges (1): 9,602\n",
      "  Clean edges (0):      69,647,964\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 6: Build Formatted Transactions File\n",
    "# ================================================================\n",
    "# Assemble the final dataframe with columns in this order:\n",
    "#   EdgeID, from_id, to_id, Timestamp, Amount Sent, Sent Currency,\n",
    "#   Amount Received, Received Currency, Payment Format, Is Laundering\n",
    "#\n",
    "# - EdgeID = row index starting at 0\n",
    "# - Sent Currency, Received Currency, Payment Format = always 1\n",
    "#   (single-chain ETH data; kept for IBM Multi-GNN format compatibility)\n",
    "#\n",
    "# Save to: OUTPUT_DIR/formatted_transactions.parquet\n",
    "# ================================================================\n",
    "\n",
    "SENT_CURRENCY_CODE = 1\n",
    "RECEIVED_CURRENCY_CODE = 1\n",
    "PAYMENT_FORMAT_CODE = 1\n",
    "\n",
    "formatted = pd.DataFrame({\n",
    "    \"EdgeID\":            range(len(df)),\n",
    "    \"from_id\":           df[\"from_id\"].astype(int),\n",
    "    \"to_id\":             df[\"to_id\"].astype(int),\n",
    "    \"Timestamp\":         df[\"Timestamp\"].astype(int),\n",
    "    \"Amount Sent\":       df[\"Amount Sent\"].astype(float),\n",
    "    \"Sent Currency\":     SENT_CURRENCY_CODE,\n",
    "    \"Amount Received\":   df[\"Amount Received\"].astype(float),\n",
    "    \"Received Currency\": RECEIVED_CURRENCY_CODE,\n",
    "    \"Payment Format\":    PAYMENT_FORMAT_CODE,\n",
    "    \"Is Laundering\":     df[\"Is Laundering\"].astype(int),\n",
    "})\n",
    "\n",
    "formatted_path = os.path.join(OUTPUT_DIR, \"formatted_transactions.parquet\")\n",
    "formatted.to_parquet(formatted_path, index=False)\n",
    "\n",
    "print(\"Step 6 — Formatted transactions saved\")\n",
    "print(f\"  Path: {formatted_path}\")\n",
    "print(f\"  Rows: {len(formatted):,}\")\n",
    "print(f\"  Columns: {list(formatted.columns)}\")\n",
    "print()\n",
    "\n",
    "n_edges = len(formatted)  # save for Step 8\n",
    "\n",
    "del df, formatted\n",
    "gc.collect()\n",
    "print(\"Memory freed before Step 7\")"
   ],
   "metadata": {
    "id": "kmMnqOIIadvw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "79aeb2d6-3678-4601-9097-76dd689fd0af"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 6 — Formatted transactions saved\n",
      "  Path: /content/drive/MyDrive/Math_168_Folder/data/processed/formatted_transactions.parquet\n",
      "  Rows: 69,657,566\n",
      "  Columns: ['EdgeID', 'from_id', 'to_id', 'Timestamp', 'Amount Sent', 'Sent Currency', 'Amount Received', 'Received Currency', 'Payment Format', 'Is Laundering']\n",
      "\n",
      "Memory freed before Step 7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 7: Build Node Labels File\n",
    "# ================================================================\n",
    "# For each address in address_to_id:\n",
    "#   - node_id = address_to_id[address]\n",
    "#   - is_sanctioned = 1 if address in sanctioned_set, else 0\n",
    "#\n",
    "# Since contracts were filtered in Step 1, only EOA addresses remain.\n",
    "#\n",
    "# Columns: node_id, is_sanctioned\n",
    "# Save to: OUTPUT_DIR/node_labels.parquet\n",
    "# ================================================================\n",
    "\n",
    "node_labels = pd.DataFrame({\n",
    "    \"node_id\": list(address_to_id.values()),\n",
    "    \"is_sanctioned\": [1 if addr in sanctioned_set else 0 for addr in address_to_id.keys()],\n",
    "}).sort_values(\"node_id\").reset_index(drop=True)\n",
    "\n",
    "node_labels_path = os.path.join(OUTPUT_DIR, \"node_labels.parquet\")\n",
    "node_labels.to_parquet(node_labels_path, index=False)\n",
    "\n",
    "n_sanctioned = node_labels['is_sanctioned'].sum()\n",
    "n_total = len(node_labels)\n",
    "ratio = (n_total - n_sanctioned) / max(n_sanctioned, 1)\n",
    "\n",
    "print(\"Step 7 — Node labels saved\")\n",
    "print(f\"  Path: {node_labels_path}\")\n",
    "print(f\"  Rows (nodes): {n_total:,}\")\n",
    "print(f\"  Sanctioned nodes: {n_sanctioned:,}\")\n",
    "print(f\"  Non-sanctioned nodes: {n_total - n_sanctioned:,}\")\n",
    "print(f\"  Class imbalance ratio: {ratio:,.0f}:1\")\n",
    "print()"
   ],
   "metadata": {
    "id": "99vEuEsaan1W",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1d84ba9b-6092-48a3-96c7-b9ba02a00289"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 7 — Node labels saved\n",
      "  Path: /content/drive/MyDrive/Math_168_Folder/data/processed/node_labels.parquet\n",
      "  Rows (nodes): 30,378,276\n",
      "  Sanctioned nodes: 69\n",
      "  Non-sanctioned nodes: 30,378,207\n",
      "  Class imbalance ratio: 440,264:1\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 8: Create Train/Validation Split\n",
    "# ================================================================\n",
    "# Data is sorted by time (Step 3). Temporal split:\n",
    "#   - Train = first 80% of edges (chronologically earliest)\n",
    "#   - Val   = last 20% of edges (chronologically latest)\n",
    "# This prevents temporal data leakage.\n",
    "#\n",
    "# No test split — in a transfer learning setup the test set comes\n",
    "# from a different domain (different blockchain).\n",
    "#\n",
    "# Save as: OUTPUT_DIR/data_splits.json\n",
    "# ================================================================\n",
    "\n",
    "split_point = int(n_edges * 0.8)\n",
    "train_edge_ids = list(range(0, split_point))\n",
    "val_edge_ids   = list(range(split_point, n_edges))\n",
    "\n",
    "splits = {\n",
    "    \"train_edge_ids\": train_edge_ids,\n",
    "    \"val_edge_ids\": val_edge_ids,\n",
    "}\n",
    "\n",
    "splits_path = os.path.join(OUTPUT_DIR, \"data_splits.json\")\n",
    "with open(splits_path, \"w\") as f:\n",
    "    json.dump(splits, f)\n",
    "\n",
    "print(\"Step 8 — Data splits saved\")\n",
    "print(f\"  Path: {splits_path}\")\n",
    "print(f\"  Train edges: {len(train_edge_ids):,} ({len(train_edge_ids)/n_edges*100:.1f}%)\")\n",
    "print(f\"  Val edges:   {len(val_edge_ids):,} ({len(val_edge_ids)/n_edges*100:.1f}%)\")\n",
    "print()"
   ],
   "metadata": {
    "id": "HCFQwbESasYi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "32fdc51d-4fdc-49fd-f395-1b0f40fcafa0"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 8 — Data splits saved\n",
      "  Path: /content/drive/MyDrive/Math_168_Folder/data/processed/data_splits.json\n",
      "  Train edges: 55,726,052 (80.0%)\n",
      "  Val edges:   13,931,514 (20.0%)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "BcDA9JdjZqBb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pLzdh15TZklk"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
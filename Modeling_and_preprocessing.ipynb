{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Innovation-On-Blockchain/Data-Modeling/blob/main/Modeling_and_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Data Processing\n",
        "# ==============================================================\n",
        "# Input:  raw/transactions_raw.parquet, labels/labels.parquet\n",
        "# Output: processed/formatted_transactions.parquet\n",
        "#         processed/node_labels.parquet\n",
        "#         processed/data_splits.json\n",
        "# ==============================================================\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMl3_b96c--c",
        "outputId": "aeba434b-f315-4439-c9ae-92792a2dc373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "shared_folder_path = '/content/drive/MyDrive/Math_168_Folder'\n",
        "os.chdir(shared_folder_path)\n",
        "\n",
        "path = \"/content/drive/MyDrive/Math_168_Folder/data/raw/transactions_raw.parquet\"\n",
        "print(f\"File size: {os.path.getsize(path) / 1e9:.2f} GB\")\n",
        "\n",
        "meta = pq.read_metadata(path)\n",
        "print(f\"Rows: {meta.num_rows:,}\")\n",
        "print(f\"Columns: {meta.num_columns}\")"
      ],
      "metadata": {
        "id": "Ti2ziOd9pdhl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5a20ac-721d-4132-c482-921cf974a082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 5.58 GB\n",
            "Rows: 90,728,163\n",
            "Columns: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0witpEDHwiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1613238-4e9a-48e1-eb37-51539fe22c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded 90,728,163 valid rows\n",
            "Raw transactions loaded: 90,728,163 rows\n",
            "Labels loaded:           82 sanctioned addresses\n",
            "\n",
            "Empty/null from_address: 0 / 90,728,163\n",
            "Empty/null to_address:   0 / 90,728,163\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# Load raw data\n",
        "# ================================================================\n",
        "# Read the two input parquet files produced by the Rust pipeline.\n",
        "# transactions_raw columns: tx_hash, block_timestamp, from_address,\n",
        "#                           to_address, value_wei  (all strings)\n",
        "# labels columns: address, label, label_source, source_url, retrieved_at\n",
        "# ================================================================\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "path = \"/content/drive/MyDrive/Math_168_Folder/data/raw/transactions_raw.parquet\"\n",
        "\n",
        "chunks = []\n",
        "parquet_file = pq.ParquetFile(path)\n",
        "\n",
        "for batch in parquet_file.iter_batches(batch_size=1_000_000):\n",
        "    chunk = batch.to_pandas()\n",
        "    # Filter invalid rows immediately to save memory\n",
        "    chunk = chunk[\n",
        "        chunk[\"from_address\"].notna() & (chunk[\"from_address\"] != \"\") &\n",
        "        chunk[\"to_address\"].notna()   & (chunk[\"to_address\"]   != \"\")\n",
        "    ]\n",
        "    chunks.append(chunk)\n",
        "    print(f\"  Processed batch — kept {len(chunk):,} rows\", end=\"\\r\")\n",
        "\n",
        "df = pd.concat(chunks, ignore_index=True)\n",
        "del chunks  # free memory\n",
        "import gc; gc.collect()\n",
        "\n",
        "print(f\"\\nLoaded {len(df):,} valid rows\")\n",
        "\n",
        "labels = pd.read_parquet(\"/content/drive/MyDrive/Math_168_Folder/data/labels/labels.parquet\")\n",
        "\n",
        "print(f\"Raw transactions loaded: {len(df):,} rows\")\n",
        "print(f\"Labels loaded:           {len(labels):,} sanctioned addresses\")\n",
        "print()\n",
        "\n",
        "# Quick data-quality report\n",
        "n_empty_from = (df[\"from_address\"].isna() | (df[\"from_address\"] == \"\")).sum()\n",
        "n_empty_to   = (df[\"to_address\"].isna()   | (df[\"to_address\"]   == \"\")).sum()\n",
        "print(f\"Empty/null from_address: {n_empty_from:,} / {len(df):,}\")\n",
        "print(f\"Empty/null to_address:   {n_empty_to:,} / {len(df):,}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is already done in the previous part (data collection), so we can safely skip this.\n",
        "# # ================================================================\n",
        "# # Step 1: Filter Invalid Transactions\n",
        "# # ================================================================\n",
        "# # Remove rows where from_address or to_address is null or empty.\n",
        "# # These are contract-creation transactions (no recipient), which\n",
        "# # are not useful for money-flow graph analysis.\n",
        "# # ================================================================\n",
        "\n",
        "# before = len(df)\n",
        "\n",
        "# df = df[\n",
        "#     df[\"from_address\"].notna() & (df[\"from_address\"] != \"\") &\n",
        "#     df[\"to_address\"].notna()   & (df[\"to_address\"]   != \"\")\n",
        "# ]\n",
        "# df = df.reset_index(drop=True)\n",
        "\n",
        "# after = len(df)\n",
        "# print(f\"Step 1 — Filter invalid transactions\")\n",
        "# print(f\"  Before: {before:,}  After: {after:,}  Removed: {before - after:,}\")\n",
        "# print()\n",
        "\n",
        "# if after == 0:\n",
        "#     print(\"WARNING: All rows were removed (every to_address is empty).\")\n",
        "#     print(\"The Rust pipeline likely has a bug. Fix the data and re-run.\")\n",
        "#     print(\"Skipping remaining steps.\")\n",
        "#     exit(0)\n"
      ],
      "metadata": {
        "id": "QAQSFfcsadc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "794mad66oO40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862de43a-1379-4303-e8e8-8901d7e74221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 2: Create Address-to-ID Mapping\n",
        "# ================================================================\n",
        "# Collect every unique address from both from_address and to_address.\n",
        "# Sort them so the mapping is deterministic, then assign each one\n",
        "# a numeric ID starting from 0. Add from_id / to_id columns.\n",
        "# ================================================================\n",
        "\n",
        "all_addresses = pd.concat([df[\"from_address\"], df[\"to_address\"]]).unique()\n",
        "address_to_id = {addr: i for i, addr in enumerate(sorted(all_addresses))}\n",
        "\n",
        "df[\"from_id\"] = df[\"from_address\"].map(address_to_id)\n",
        "df[\"to_id\"]   = df[\"to_address\"].map(address_to_id)\n",
        "\n",
        "print(f\"Step 2 — Address-to-ID mapping\")\n",
        "print(f\"  Unique addresses: {len(address_to_id):,}\")\n",
        "print(f\"  ID range: 0 .. {len(address_to_id) - 1}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "Q_hbptc1admQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b032fb78-548e-446d-9d0f-a46bd661d33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2 — Address-to-ID mapping\n",
            "  Unique addresses: 33,895,535\n",
            "  ID range: 0 .. 33895534\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 3: Convert Timestamps\n",
        "# ================================================================\n",
        "# Parse block_timestamp from ISO-8601 strings to datetime objects.\n",
        "# Sort the dataframe by timestamp ascending (earliest first).\n",
        "# Subtract the earliest timestamp so the first row has Timestamp=0\n",
        "# and all other values are seconds elapsed since that first tx.\n",
        "# ================================================================\n",
        "\n",
        "df[\"block_timestamp\"] = pd.to_datetime(df[\"block_timestamp\"])\n",
        "df = df.sort_values(\"block_timestamp\").reset_index(drop=True)\n",
        "\n",
        "t_min = df[\"block_timestamp\"].min()\n",
        "df[\"Timestamp\"] = (df[\"block_timestamp\"] - t_min).dt.total_seconds().astype(int)\n",
        "\n",
        "print(f\"Step 3 — Convert timestamps\")\n",
        "print(f\"  Earliest: {t_min}\")\n",
        "print(f\"  Latest:   {df['block_timestamp'].max()}\")\n",
        "print(f\"  Span:     {df['Timestamp'].max():,} seconds\")\n",
        "print(f\"  First Timestamp value: {df['Timestamp'].iloc[0]}\")\n",
        "print()\n",
        "\n",
        "df.drop(columns=[\"block_timestamp\"], inplace=True)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "A4-jqebmadol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d8da5d-2b81-4d2f-ab0b-8aa86328259d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3 — Convert timestamps\n",
            "  Earliest: 2015-08-07 05:01:09+00:00\n",
            "  Latest:   2026-02-19 19:21:47+00:00\n",
            "  Span:     332,605,238 seconds\n",
            "  First Timestamp value: 0\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 4: Convert Values (Wei -> ETH)\n",
        "# ================================================================\n",
        "# value_wei is a string in wei (1 ETH = 1e18 wei).\n",
        "# Convert to float ETH. Store as both \"Amount Sent\" and\n",
        "# \"Amount Received\" — they are equal for direct ETH transfers.\n",
        "# ================================================================\n",
        "\n",
        "WEI_PER_ETH = 10**18\n",
        "\n",
        "df[\"Amount Sent\"]     = df[\"value_wei\"].astype(float) / WEI_PER_ETH\n",
        "df[\"Amount Received\"] = df[\"Amount Sent\"]\n",
        "\n",
        "print(f\"Step 4 — Wei to ETH conversion\")\n",
        "print(f\"  Min:  {df['Amount Sent'].min():.8f} ETH\")\n",
        "print(f\"  Max:  {df['Amount Sent'].max():.8f} ETH\")\n",
        "print(f\"  Mean: {df['Amount Sent'].mean():.8f} ETH\")\n",
        "print()\n",
        "\n",
        "df.drop(columns=[\"value_wei\"], inplace=True)\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "FVGu7ozBadrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6afda9f0-2872-44b8-be5f-081826d87c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4 — Wei to ETH conversion\n",
            "  Min:  0.00000000 ETH\n",
            "  Max:  1400000.00000000 ETH\n",
            "  Mean: 14.01213088 ETH\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 5: Add Edge Labels\n",
        "# ================================================================\n",
        "# Build a set of the 82 OFAC-sanctioned addresses from labels.\n",
        "# For each transaction edge:\n",
        "#   - Is Laundering = 1  if from_address OR to_address is sanctioned\n",
        "#   - Is Laundering = 0  if NEITHER endpoint is sanctioned\n",
        "# Edges between two clean addresses are intentionally kept as 0 —\n",
        "# they give the GNN examples of normal graph structure.\n",
        "# ================================================================\n",
        "\n",
        "sanctioned_set = set(labels[\"address\"])\n",
        "\n",
        "df[\"Is Laundering\"] = (\n",
        "    df[\"from_address\"].isin(sanctioned_set) |\n",
        "    df[\"to_address\"].isin(sanctioned_set)\n",
        ").astype(int)\n",
        "\n",
        "n_launder = (df[\"Is Laundering\"] == 1).sum()\n",
        "n_clean   = (df[\"Is Laundering\"] == 0).sum()\n",
        "print(f\"Step 5 — Edge labels\")\n",
        "print(f\"  Sanctioned addresses: {len(sanctioned_set)}\")\n",
        "print(f\"  Laundering edges (1): {n_launder:,}\")\n",
        "print(f\"  Clean edges (0):      {n_clean:,}\")\n",
        "print()\n",
        "\n",
        "df.drop(columns=[\"from_address\", \"to_address\"], inplace=True)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "6Or1258Hadtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73489bca-f166-4ea0-dadc-b33501bbf910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5 — Edge labels\n",
            "  Sanctioned addresses: 82\n",
            "  Laundering edges (1): 10,453\n",
            "  Clean edges (0):      90,717,710\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 6: Build Formatted Transactions File\n",
        "# ================================================================\n",
        "# TODO: Assemble the final dataframe with columns in this order:\n",
        "#   EdgeID, from_id, to_id, Timestamp, Amount Sent, Sent Currency,\n",
        "#   Amount Received, Received Currency, Payment Format, Is Laundering\n",
        "#\n",
        "# - EdgeID = row index starting at 0\n",
        "# - Sent Currency, Received Currency, Payment Format = always 1\n",
        "#\n",
        "# Save to: processed/formatted_transactions.parquet\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "SENT_CURRENCY_CODE = 1\n",
        "RECEIVED_CURRENCY_CODE = 1\n",
        "PAYMENT_FORMAT_CODE = 1\n",
        "\n",
        "formatted = pd.DataFrame({\n",
        "    \"EdgeID\":            range(len(df)),\n",
        "    \"from_id\":           df[\"from_id\"].astype(int),\n",
        "    \"to_id\":             df[\"to_id\"].astype(int),\n",
        "    \"Timestamp\":         df[\"Timestamp\"].astype(int),\n",
        "    \"Amount Sent\":       df[\"Amount Sent\"].astype(float),\n",
        "    \"Sent Currency\":     SENT_CURRENCY_CODE,\n",
        "    \"Amount Received\":   df[\"Amount Received\"].astype(float),\n",
        "    \"Received Currency\": RECEIVED_CURRENCY_CODE,\n",
        "    \"Payment Format\":    PAYMENT_FORMAT_CODE,\n",
        "    \"Is Laundering\":     df[\"Is Laundering\"].astype(int),\n",
        "})\n",
        "\n",
        "# Ensure the processed folder exists\n",
        "processed_dir = \"/content/drive/MyDrive/Math_168_Folder/data/processed\"\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "formatted_path = os.path.join(processed_dir, \"formatted_transactions.parquet\")\n",
        "formatted.to_parquet(formatted_path, index=False)\n",
        "\n",
        "print(\"Step 6 — Formatted transactions saved\")\n",
        "print(f\"  Path: {formatted_path}\")\n",
        "print(f\"  Rows: {len(formatted):,}\")\n",
        "print(f\"  Columns: {list(formatted.columns)}\")\n",
        "print()\n",
        "\n",
        "del df, formatted\n",
        "gc.collect()\n",
        "print(\"Memory freed before Step 7\")"
      ],
      "metadata": {
        "id": "kmMnqOIIadvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e31c529-a2f0-4800-9916-d045923db562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6 — Formatted transactions saved\n",
            "  Path: /content/drive/MyDrive/Math_168_Folder/data/processed/formatted_transactions.parquet\n",
            "  Rows: 90,728,163\n",
            "  Columns: ['EdgeID', 'from_id', 'to_id', 'Timestamp', 'Amount Sent', 'Sent Currency', 'Amount Received', 'Received Currency', 'Payment Format', 'Is Laundering']\n",
            "\n",
            "Memory freed before Step 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 7: Build Node Labels File\n",
        "# ================================================================\n",
        "# TODO: For each address in address_to_id:\n",
        "#   - node_id = address_to_id[address]\n",
        "#   - is_sanctioned = 1 if address in sanctioned_set, else 0\n",
        "#\n",
        "# Columns: node_id, is_sanctioned\n",
        "# Save to: processed/node_labels.parquet\n",
        "# ================================================================\n",
        "\n",
        "node_labels = pd.DataFrame({\n",
        "    \"node_id\": list(address_to_id.values()),\n",
        "    \"is_sanctioned\": [1 if addr in sanctioned_set else 0 for addr in address_to_id.keys()],\n",
        "}).sort_values(\"node_id\").reset_index(drop=True)\n",
        "\n",
        "node_labels_path = os.path.join(processed_dir, \"node_labels.parquet\")\n",
        "node_labels.to_parquet(node_labels_path, index=False)\n",
        "\n",
        "print(\"Step 7 — Node labels saved\")\n",
        "print(f\"  Path: {node_labels_path}\")\n",
        "print(f\"  Rows (nodes): {len(node_labels):,}\")\n",
        "print(f\"  Sanctioned nodes: {node_labels['is_sanctioned'].sum():,}\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "99vEuEsaan1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d42439-0547-44c6-a126-6585bd58b59a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7 — Node labels saved\n",
            "  Path: /content/drive/MyDrive/Math_168_Folder/data/processed/node_labels.parquet\n",
            "  Rows (nodes): 33,895,535\n",
            "  Sanctioned nodes: 75\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 8: Create Train/Validation Split\n",
        "# ================================================================\n",
        "# TODO: Data is sorted by time (Step 3).\n",
        "#   - split_point = int(len(formatted) * 0.8)\n",
        "#   - train_edge_ids = list(range(0, split_point))\n",
        "#   - val_edge_ids   = list(range(split_point, len(formatted)))\n",
        "#\n",
        "# Save as processed/data_splits.json:\n",
        "#   {\"train_edge_ids\": [...], \"val_edge_ids\": [...]}\n",
        "# ================================================================\n",
        "n_edges = 90_728_163\n",
        "split_point = int(n_edges * 0.8)\n",
        "train_edge_ids = list(range(0, split_point))\n",
        "val_edge_ids   = list(range(split_point, n_edges))\n",
        "\n",
        "splits = {\n",
        "    \"train_edge_ids\": train_edge_ids,\n",
        "    \"val_edge_ids\": val_edge_ids,\n",
        "}\n",
        "\n",
        "splits_path = os.path.join(processed_dir, \"data_splits.json\")\n",
        "with open(splits_path, \"w\") as f:\n",
        "    json.dump(splits, f)\n",
        "\n",
        "print(\"Step 8 — Data splits saved\")\n",
        "print(f\"  Path: {splits_path}\")\n",
        "print(f\"  Train edges: {len(train_edge_ids):,}\")\n",
        "print(f\"  Val edges:   {len(val_edge_ids):,}\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "HCFQwbESasYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d71784-5f24-4643-a3ab-def67ee8ff29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 8 — Data splits saved\n",
            "  Path: /content/drive/MyDrive/Math_168_Folder/data/processed/data_splits.json\n",
            "  Train edges: 72,582,530\n",
            "  Val edges:   18,145,633\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BcDA9JdjZqBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLzdh15TZklk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#TODO: Data Processing\n",
        "# ==============================================================\n",
        "# Input:  raw/transactions_raw.parquet, labels/labels.parquet,\n",
        "#         processed/nodes.parquet (for contract filtering)\n",
        "# Output: processed/formatted_transactions.parquet\n",
        "#         processed/node_labels.parquet\n",
        "#         processed/data_splits.json\n",
        "# ==============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import gc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMl3_b96c--c",
        "outputId": "e46260cb-56eb-4ea1-89b0-132e4d6767d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "shared_folder_path = '/content/drive/MyDrive/Math_168_Folder'\n",
        "os.chdir(shared_folder_path)\n",
        "DATA_DIR = \"/content/drive/MyDrive/Math_168_Folder/data\"\n",
        "OUTPUT_DIR = os.path.join(DATA_DIR, \"processed\")\n",
        "\n",
        "# Input files (from Rust pipeline)\n",
        "TRANSACTIONS_PATH = os.path.join(DATA_DIR, \"raw\", \"transactions_raw.parquet\")\n",
        "LABELS_PATH       = os.path.join(DATA_DIR, \"labels\", \"labels.parquet\")\n",
        "NODES_PATH        = os.path.join(DATA_DIR, \"processed\", \"nodes.parquet\")\n",
        "\n",
        "print(f\"File size: {os.path.getsize(TRANSACTIONS_PATH) / 1e9:.2f} GB\")\n",
        "\n",
        "meta = pq.read_metadata(TRANSACTIONS_PATH)\n",
        "print(f\"Rows: {meta.num_rows:,}\")\n",
        "print(f\"Columns: {meta.num_columns}\")"
      ],
      "metadata": {
        "id": "Ti2ziOd9pdhl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce96313-d237-45d6-8958-58c33f0ce4d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 5.58 GB\n",
            "Rows: 90,728,163\n",
            "Columns: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W0witpEDHwiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f888e01-b074-4bbd-c23a-c909c29a0e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded 90,728,163 valid rows\n",
            "Raw transactions loaded: 90,728,163 rows\n",
            "Labels loaded:           82 sanctioned addresses\n",
            "\n",
            "Empty/null from_address: 0 / 90,728,163\n",
            "Empty/null to_address:   0 / 90,728,163\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# Load raw data\n",
        "# ================================================================\n",
        "# Read the two input parquet files produced by the Rust pipeline.\n",
        "# transactions_raw columns: tx_hash, block_timestamp, from_address,\n",
        "#                           to_address, value_wei  (all strings)\n",
        "# labels columns: address, label, label_source, source_url, retrieved_at\n",
        "# ================================================================\n",
        "\n",
        "chunks = []\n",
        "parquet_file = pq.ParquetFile(TRANSACTIONS_PATH)\n",
        "\n",
        "for batch in parquet_file.iter_batches(batch_size=1_000_000):\n",
        "    chunk = batch.to_pandas()\n",
        "    # Filter invalid rows immediately to save memory\n",
        "    chunk = chunk[\n",
        "        chunk[\"from_address\"].notna() & (chunk[\"from_address\"] != \"\") &\n",
        "        chunk[\"to_address\"].notna()   & (chunk[\"to_address\"]   != \"\")\n",
        "    ]\n",
        "    chunks.append(chunk)\n",
        "    print(f\"  Processed batch — kept {len(chunk):,} rows\", end=\"\\r\")\n",
        "\n",
        "df = pd.concat(chunks, ignore_index=True)\n",
        "del chunks\n",
        "gc.collect()\n",
        "\n",
        "print(f\"\\nLoaded {len(df):,} valid rows\")\n",
        "\n",
        "labels = pd.read_parquet(LABELS_PATH)\n",
        "\n",
        "print(f\"Raw transactions loaded: {len(df):,} rows\")\n",
        "print(f\"Labels loaded:           {len(labels):,} sanctioned addresses\")\n",
        "print()\n",
        "\n",
        "# Quick data-quality report\n",
        "n_empty_from = (df[\"from_address\"].isna() | (df[\"from_address\"] == \"\")).sum()\n",
        "n_empty_to   = (df[\"to_address\"].isna()   | (df[\"to_address\"]   == \"\")).sum()\n",
        "print(f\"Empty/null from_address: {n_empty_from:,} / {len(df):,}\")\n",
        "print(f\"Empty/null to_address:   {n_empty_to:,} / {len(df):,}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 1: Filter Out Contract Addresses\n",
        "# ================================================================\n",
        "# Load nodes.parquet from the Rust pipeline (Stage 4: build-nodes).\n",
        "# It contains: address, is_contract (bool), degree_in, degree_out, node_type.\n",
        "# Remove all transaction rows where EITHER from_address or to_address\n",
        "# is a smart contract. This keeps only EOA-to-EOA transactions.\n",
        "# ================================================================\n",
        "\n",
        "nodes_pf = pq.ParquetFile(NODES_PATH)\n",
        "\n",
        "# Only read the columns we need to save memory\n",
        "contract_addresses = set()\n",
        "for batch in nodes_pf.iter_batches(batch_size=1_000_000, columns=[\"address\", \"is_contract\"]):\n",
        "    chunk = batch.to_pandas()\n",
        "    contracts = chunk.loc[chunk[\"is_contract\"] == True, \"address\"]\n",
        "    contract_addresses.update(contracts)\n",
        "    print(f\"  Scanned batch — found {len(contract_addresses):,} contracts so far\", end=\"\\r\")\n",
        "\n",
        "print(f\"\\nTotal contract addresses found: {len(contract_addresses):,}\")\n",
        "\n",
        "before = len(df)\n",
        "df = df[\n",
        "    ~df[\"from_address\"].isin(contract_addresses) &\n",
        "    ~df[\"to_address\"].isin(contract_addresses)\n",
        "]\n",
        "df = df.reset_index(drop=True)\n",
        "after = len(df)\n",
        "\n",
        "print(f\"\\nStep 1 — Filter contract addresses\")\n",
        "print(f\"  Before: {before:,}  After: {after:,}  Removed: {before - after:,}\")\n",
        "print(f\"  Percentage removed: {(before - after) / before * 100:.2f}%\")\n",
        "print()\n",
        "\n",
        "del contract_addresses\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "QAQSFfcsadc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65cfaf65-dd97-4ed4-82e7-7de586f1e693"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Scanned batch — found 721,891 contracts so far\n",
            "Total contract addresses found: 721,891\n",
            "\n",
            "Step 1 — Filter contract addresses\n",
            "  Before: 90,728,163  After: 69,657,566  Removed: 21,070,597\n",
            "  Percentage removed: 23.22%\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "794mad66oO40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7800611-4d83-41bf-bdb5-793fade5ac1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 2: Create Address-to-ID Mapping\n",
        "# ================================================================\n",
        "# Collect every unique address from both from_address and to_address.\n",
        "# Sort them so the mapping is deterministic, then assign each one\n",
        "# a numeric ID starting from 0. Add from_id / to_id columns.\n",
        "# ================================================================\n",
        "\n",
        "all_addresses = pd.concat([df[\"from_address\"], df[\"to_address\"]]).unique()\n",
        "address_to_id = {addr: i for i, addr in enumerate(sorted(all_addresses))}\n",
        "\n",
        "df[\"from_id\"] = df[\"from_address\"].map(address_to_id)\n",
        "df[\"to_id\"]   = df[\"to_address\"].map(address_to_id)\n",
        "\n",
        "print(f\"Step 2 — Address-to-ID mapping\")\n",
        "print(f\"  Unique addresses: {len(address_to_id):,}\")\n",
        "print(f\"  ID range: 0 .. {len(address_to_id) - 1}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "Q_hbptc1admQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9337a64c-7551-43c5-e0e1-b1e64fa4029d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2 — Address-to-ID mapping\n",
            "  Unique addresses: 30,378,276\n",
            "  ID range: 0 .. 30378275\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 3: Convert Timestamps\n",
        "# ================================================================\n",
        "# Parse block_timestamp from ISO-8601 strings to datetime objects.\n",
        "# Sort the dataframe by timestamp ascending (earliest first).\n",
        "# Subtract the earliest timestamp so the first row has Timestamp=0\n",
        "# and all other values are seconds elapsed since that first tx.\n",
        "# ================================================================\n",
        "\n",
        "df[\"block_timestamp\"] = pd.to_datetime(df[\"block_timestamp\"])\n",
        "df = df.sort_values(\"block_timestamp\").reset_index(drop=True)\n",
        "\n",
        "t_min = df[\"block_timestamp\"].min()\n",
        "df[\"Timestamp\"] = (df[\"block_timestamp\"] - t_min).dt.total_seconds().astype(int)\n",
        "\n",
        "print(f\"Step 3 — Convert timestamps\")\n",
        "print(f\"  Earliest: {t_min}\")\n",
        "print(f\"  Latest:   {df['block_timestamp'].max()}\")\n",
        "print(f\"  Span:     {df['Timestamp'].max():,} seconds\")\n",
        "print(f\"  First Timestamp value: {df['Timestamp'].iloc[0]}\")\n",
        "print()\n",
        "\n",
        "df.drop(columns=[\"block_timestamp\"], inplace=True)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "A4-jqebmadol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a91308-6f99-4d9b-ea76-6040fe4fd9bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3 — Convert timestamps\n",
            "  Earliest: 2015-08-07 05:01:09+00:00\n",
            "  Latest:   2026-02-19 19:21:47+00:00\n",
            "  Span:     332,605,238 seconds\n",
            "  First Timestamp value: 0\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 4: Convert Values (Wei -> ETH)\n",
        "# ================================================================\n",
        "# value_wei is a string in wei (1 ETH = 1e18 wei).\n",
        "# Convert to float ETH. Store as both \"Amount Sent\" and\n",
        "# \"Amount Received\" — they are equal for direct ETH transfers.\n",
        "# ================================================================\n",
        "\n",
        "WEI_PER_ETH = 10**18\n",
        "\n",
        "df[\"Amount Sent\"]     = df[\"value_wei\"].astype(float) / WEI_PER_ETH\n",
        "df[\"Amount Received\"] = df[\"Amount Sent\"]\n",
        "\n",
        "print(f\"Step 4 — Wei to ETH conversion\")\n",
        "print(f\"  Min:  {df['Amount Sent'].min():.8f} ETH\")\n",
        "print(f\"  Max:  {df['Amount Sent'].max():.8f} ETH\")\n",
        "print(f\"  Mean: {df['Amount Sent'].mean():.8f} ETH\")\n",
        "print()\n",
        "\n",
        "df.drop(columns=[\"value_wei\"], inplace=True)\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "FVGu7ozBadrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e0e085-caba-4284-bd55-b5b900fb86e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4 — Wei to ETH conversion\n",
            "  Min:  0.00000000 ETH\n",
            "  Max:  1400000.00000000 ETH\n",
            "  Mean: 14.61243847 ETH\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 5: Add Edge Labels\n",
        "# ================================================================\n",
        "# Build a set of the 82 OFAC-sanctioned addresses from labels.\n",
        "# For each transaction edge:\n",
        "#   - Is Laundering = 1  if from_address OR to_address is sanctioned\n",
        "#   - Is Laundering = 0  if NEITHER endpoint is sanctioned\n",
        "# Edges between two clean addresses are intentionally kept as 0 —\n",
        "# they give the GNN examples of normal graph structure.\n",
        "# ================================================================\n",
        "\n",
        "sanctioned_set = set(labels[\"address\"])\n",
        "\n",
        "df[\"Is Laundering\"] = (\n",
        "    df[\"from_address\"].isin(sanctioned_set) |\n",
        "    df[\"to_address\"].isin(sanctioned_set)\n",
        ").astype(int)\n",
        "\n",
        "n_launder = (df[\"Is Laundering\"] == 1).sum()\n",
        "n_clean   = (df[\"Is Laundering\"] == 0).sum()\n",
        "print(f\"Step 5 — Edge labels\")\n",
        "print(f\"  Sanctioned addresses: {len(sanctioned_set)}\")\n",
        "print(f\"  Laundering edges (1): {n_launder:,}\")\n",
        "print(f\"  Clean edges (0):      {n_clean:,}\")\n",
        "print()\n",
        "\n",
        "df.drop(columns=[\"from_address\", \"to_address\"], inplace=True)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "6Or1258Hadtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040d7a34-f140-49e9-9024-9691b09456eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5 — Edge labels\n",
            "  Sanctioned addresses: 82\n",
            "  Laundering edges (1): 9,602\n",
            "  Clean edges (0):      69,647,964\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 6: Build Formatted Transactions File\n",
        "# ================================================================\n",
        "# Assemble the final dataframe with columns in this order:\n",
        "#   EdgeID, from_id, to_id, Timestamp, Amount Sent, Sent Currency,\n",
        "#   Amount Received, Received Currency, Payment Format, Is Laundering\n",
        "#\n",
        "# - EdgeID = row index starting at 0\n",
        "# - Sent Currency, Received Currency, Payment Format = always 1\n",
        "#   (single-chain ETH data; kept for IBM Multi-GNN format compatibility)\n",
        "#\n",
        "# Save to: OUTPUT_DIR/formatted_transactions.parquet\n",
        "# ================================================================\n",
        "\n",
        "SENT_CURRENCY_CODE = 1\n",
        "RECEIVED_CURRENCY_CODE = 1\n",
        "PAYMENT_FORMAT_CODE = 1\n",
        "\n",
        "formatted = pd.DataFrame({\n",
        "    \"EdgeID\":            range(len(df)),\n",
        "    \"from_id\":           df[\"from_id\"].astype(int),\n",
        "    \"to_id\":             df[\"to_id\"].astype(int),\n",
        "    \"Timestamp\":         df[\"Timestamp\"].astype(int),\n",
        "    \"Amount Sent\":       df[\"Amount Sent\"].astype(float),\n",
        "    \"Sent Currency\":     SENT_CURRENCY_CODE,\n",
        "    \"Amount Received\":   df[\"Amount Received\"].astype(float),\n",
        "    \"Received Currency\": RECEIVED_CURRENCY_CODE,\n",
        "    \"Payment Format\":    PAYMENT_FORMAT_CODE,\n",
        "    \"Is Laundering\":     df[\"Is Laundering\"].astype(int),\n",
        "})\n",
        "\n",
        "formatted_path = os.path.join(OUTPUT_DIR, \"formatted_transactions.parquet\")\n",
        "formatted.to_parquet(formatted_path, index=False)\n",
        "\n",
        "print(\"Step 6 — Formatted transactions saved\")\n",
        "print(f\"  Path: {formatted_path}\")\n",
        "print(f\"  Rows: {len(formatted):,}\")\n",
        "print(f\"  Columns: {list(formatted.columns)}\")\n",
        "print()\n",
        "\n",
        "n_edges = len(formatted)  # save for Step 8\n",
        "\n",
        "del df, formatted\n",
        "gc.collect()\n",
        "print(\"Memory freed before Step 7\")"
      ],
      "metadata": {
        "id": "kmMnqOIIadvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79aeb2d6-3678-4601-9097-76dd689fd0af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6 — Formatted transactions saved\n",
            "  Path: /content/drive/MyDrive/Math_168_Folder/data/processed/formatted_transactions.parquet\n",
            "  Rows: 69,657,566\n",
            "  Columns: ['EdgeID', 'from_id', 'to_id', 'Timestamp', 'Amount Sent', 'Sent Currency', 'Amount Received', 'Received Currency', 'Payment Format', 'Is Laundering']\n",
            "\n",
            "Memory freed before Step 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 7: Build Node Labels File\n",
        "# ================================================================\n",
        "# For each address in address_to_id:\n",
        "#   - node_id = address_to_id[address]\n",
        "#   - is_sanctioned = 1 if address in sanctioned_set, else 0\n",
        "#\n",
        "# Since contracts were filtered in Step 1, only EOA addresses remain.\n",
        "#\n",
        "# Columns: node_id, is_sanctioned\n",
        "# Save to: OUTPUT_DIR/node_labels.parquet\n",
        "# ================================================================\n",
        "\n",
        "node_labels = pd.DataFrame({\n",
        "    \"node_id\": list(address_to_id.values()),\n",
        "    \"is_sanctioned\": [1 if addr in sanctioned_set else 0 for addr in address_to_id.keys()],\n",
        "}).sort_values(\"node_id\").reset_index(drop=True)\n",
        "\n",
        "node_labels_path = os.path.join(OUTPUT_DIR, \"node_labels.parquet\")\n",
        "node_labels.to_parquet(node_labels_path, index=False)\n",
        "\n",
        "n_sanctioned = node_labels['is_sanctioned'].sum()\n",
        "n_total = len(node_labels)\n",
        "ratio = (n_total - n_sanctioned) / max(n_sanctioned, 1)\n",
        "\n",
        "print(\"Step 7 — Node labels saved\")\n",
        "print(f\"  Path: {node_labels_path}\")\n",
        "print(f\"  Rows (nodes): {n_total:,}\")\n",
        "print(f\"  Sanctioned nodes: {n_sanctioned:,}\")\n",
        "print(f\"  Non-sanctioned nodes: {n_total - n_sanctioned:,}\")\n",
        "print(f\"  Class imbalance ratio: {ratio:,.0f}:1\")\n",
        "print()"
      ],
      "metadata": {
        "id": "99vEuEsaan1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d84ba9b-6092-48a3-96c7-b9ba02a00289"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7 — Node labels saved\n",
            "  Path: /content/drive/MyDrive/Math_168_Folder/data/processed/node_labels.parquet\n",
            "  Rows (nodes): 30,378,276\n",
            "  Sanctioned nodes: 69\n",
            "  Non-sanctioned nodes: 30,378,207\n",
            "  Class imbalance ratio: 440,264:1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Step 8: Create Train/Validation Split\n",
        "# ================================================================\n",
        "# Data is sorted by time (Step 3). Temporal split:\n",
        "#   - Train = first 80% of edges (chronologically earliest)\n",
        "#   - Val   = last 20% of edges (chronologically latest)\n",
        "# This prevents temporal data leakage.\n",
        "#\n",
        "# No test split — in a transfer learning setup the test set comes\n",
        "# from a different domain (different blockchain).\n",
        "#\n",
        "# Save as: OUTPUT_DIR/data_splits.json\n",
        "# ================================================================\n",
        "\n",
        "split_point = int(n_edges * 0.8)\n",
        "train_edge_ids = list(range(0, split_point))\n",
        "val_edge_ids   = list(range(split_point, n_edges))\n",
        "\n",
        "splits = {\n",
        "    \"train_edge_ids\": train_edge_ids,\n",
        "    \"val_edge_ids\": val_edge_ids,\n",
        "}\n",
        "\n",
        "splits_path = os.path.join(OUTPUT_DIR, \"data_splits.json\")\n",
        "with open(splits_path, \"w\") as f:\n",
        "    json.dump(splits, f)\n",
        "\n",
        "print(\"Step 8 — Data splits saved\")\n",
        "print(f\"  Path: {splits_path}\")\n",
        "print(f\"  Train edges: {len(train_edge_ids):,} ({len(train_edge_ids)/n_edges*100:.1f}%)\")\n",
        "print(f\"  Val edges:   {len(val_edge_ids):,} ({len(val_edge_ids)/n_edges*100:.1f}%)\")\n",
        "print()"
      ],
      "metadata": {
        "id": "HCFQwbESasYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32fdc51d-4fdc-49fd-f395-1b0f40fcafa0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 8 — Data splits saved\n",
            "  Path: /content/drive/MyDrive/Math_168_Folder/data/processed/data_splits.json\n",
            "  Train edges: 55,726,052 (80.0%)\n",
            "  Val edges:   13,931,514 (20.0%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BcDA9JdjZqBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLzdh15TZklk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}